Docker  - create image for your application 
	    ( for each microservice ) - (like code for sign in , profile, home )
	    ( so that it can run on for any user )
		- docker image running == docker container


Monolothic application  = all code in one file
Microservice applicaion = code splitted into multiple files, 
                          so that respective changes can occur 
				  for particular file

Kubernetes = container automation  - Orchestration
Docker Swarm = container orchestration

so for each microservice , we create containers and inorder to 
manage all containers, we store them in pods ( pods = containers )
and deploy and manage them inside a kubernetes cluster ( k8s )
 - which include master node , worker nodes, etc.
 - in kubernetes, data volumnes can share data in same pod,but not within different pods
 - best practice, to put  one container in one pod , else there is no limiation on creation of pods


'''''''''''''''''''''''''''''''''''''''''''KUBERNETES'''''''''''''''''''''''''''''''''''''''

what is kubernetes pod ?
pod is the smallest computing unit that contains one or more 
 container which run in a kubernetes cluster
- can have one or more containers, Volumes, and Environments
- have unique IP addres
- Pods cant communicate with each other so we use services for that 


Q Why should we deploy multiple containers in one pod ?
- If app is for web content, we divide app into 2 containers , 
  one for web content and other for 		# ECOMMERCE
  other stuff like images, videos, etc.		# LOGS
- containes can read this token which is in shared volume 
	and get the secret/passwords needed
- 




MASTER
api-server 	       = access to k8s cluster  (kubectl)
controller-manager = keeps tracks of what is happening in cluster (thus manages replicas of the pods , if one dies schedules command to create new one instead where actual state == desired state)
scheduler   	   = ensures Pods placement
etcd		       = key:value backing storage

WORKER
each pods is its  own self-contained server , where they can communicate with each
 other using internal ip addresses of each POD
- when pod dies , new pod created with new IP addresses.  (this is inconvenient )
- so  instead of dynamic IPaddresess, we use service as alternative instead of Ip addresess. (sitting in front of each pod )
[so if pod dies , the new pod gets the same service ]
SERVICES USES :-
	- which gives permanent IP address
	- load balancer

how does we create kubernetes configuration ?
- goes through API server in MASTER 
[ we can communicate with api server  using UI , API (python script) ,CLI (kubectl) ] 
  > which sends configuration request to the api server
  .(request needs to be either YAML or JSON)

DEPLOYEMENT
a template or blueprint for creating PODS.(like replicas:2)

INGRESS
provides routing rules to manage external users' access to the services in a Kubernetes cluster, typically via HTTPS/HTTP
( use for port-forwarding to the service )

configMAp
helps in changing endpoints of the service dynamically

SECRET:
	used to store secret credentials , in base64 encoded - username and password

PODS, SERVICES , INGRESS , CONFIGMAP , SECRET      are all kind in yaml file called kubernetes OBJECTS

if db is restarted , data is lost , we use VOLUMES 
it attaches pyhsical storage on the POD (either on local machone or remote storage like cloud )

The replica is connected to the same service 
- a service is like a persistent static IP address with a DNS name , so that we dont need to do changes constantly (like if pod dies , change ip address of pods)

LABELS
- used to tag pods , services , ingress , configmap , secret
- used to identify objects in the cluster
- used to map these objects to other objects



3 WORKER NODE PROCESSES:
KUBELET    - interacts with both container and node  starts POD with a container inside
|			 nodes interact with each other using SERVICES 
KUBE PROXY - makes sure communication flow create low head on node 
|			(eg if request is made on replica's db , replicas's db forward request to primary node's db ,THUS avoiding network overhead )
|			 Container runtime like docker or any conatiner

Q HOW PROCESS   WORKS IN KUBERNETES ?

	user request to  > 
	api server (ENTRYPOINT) > 
	validate request > 
	Scheduler (schedules commands on PODS intelligently) > Kubelet ( executes here (in worker node) ) >
	Controller Manager (detects cluster state changes )  > Scheduler > Kubelet 
	etcd > cluster brain , all changes stored in key:value storage # here application data is NOT STORED 


'''''''''''''''''''''''''''''''''''''''''''''''''' CHUCK '''''''''''''''''''''''''''''''''''''''''''''''''''''''

container orchestration == kubernetes
any container !

ONE master node > handles all worker nodes
- These worker nodes are placed under "PODS"
- master having
  kubernetes API server   ( kubectl for commands   ),
  Scheduler, 
  Controller Manager, 
  etcd


worker nodes  have  components >
  kubelet
  kube-proxy
  container Docker
  OS (Ubuntu)
  Hardware





# 					USE CASE 
# LOCAL SERVER
# download latest kubectl
# chmod +x ./kubectl
# mv ./kubectl /usr/local/bin/kubectl
# vi kubeconfig.yaml
# export KUBECONFIG=kubeconfig.yml


# AWS 
  handles MASTER NODE
  self managed by aws , use kubernetes API Endpoint

# download latest kubectl
# chmod +x ./kubectl
# mv ./kubectl /usr/local/bin/kubectl
# vi kubeconfig.yaml
# export KUBECONFIG=kubeconfig.yaml
# kubectl get nodes
# kubectl cluster-info
# kubectl run image_name --image=image_name/path --port=80
# kubectl get pods 		  ( -- container created )
# kubectl describe pods         ( describe pod information )

 (Deployment of multiple containers in different pods)

<<< deployement.yaml >>>
 
kind:deployment
metadata:
	name: name_of_the_deployement
	labels:
	app:ncoffee
specs:
	replicas:3
	...
	spec:
		container:
			name:nccoffee 		# container name
			image:docker_image_name # from HUB
			ports:
				-containerPort:80 # where website runs

<<<  create this file  and paste in server  >>>

# kubectl delete pods pod_name        ( handling only one pod )
# kubectl apply -f deployment.yaml    ( this yaml will deploy the replicas of the container and manage it)
# kubectl get pods			  ( 3 pods created )

(We make sure we have 3 pods for sure == so edit Deployement)

# kubectl edit deployment deployment_yaml_name 
( we add 10 replicas in yaml file instead 3) >>> ( 10 pods created )

( master node's scheduler will check which worker node is busy or not 
 and assign jobs, so multiple nodes in multiple pods are handled )

( inorder to expose pods ip , we need service == Loadbalancer )

( create service.yaml )

apiversion:v1
kind:service
metadata:
	name:service
spec:
	type:LoadBalancer
	ports:
		name:http
		port:80
		protocol :TCP
		targetport:80
	selector:
		app:ncoffee			# handle pods which has this app lable 'ncoffee' under metadata of deployement.yaml

# kubectl apply -f service.yaml	# LOADBALANCER CREATED in pods


Q WE need to to scale up pods based on metrics of cluster of pod  !?
 
# create a service deployement yaml file and apply kubectl 