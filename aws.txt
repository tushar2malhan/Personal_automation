VPC
  - vpc is isolated network in a Region
   ( cidr = 10.0.0.0/16 )   
  - there can be one vpc per AZ ( Availability zone )
  - thus we divide the vpc into multiple subnets (public & private) in AZ
    ( cidr1 = 10.0.1.0/24  cidr2 = 10.0.0.0/24 )
    ( cidr is notation block to distribute Ip addresses into subnets )
  - 10.0.1.0/24  means 10.0.1.0 are network bits  each one called Octects ranging from 0-255 
  - /24 means Host ID = so out of 4 octects, 3 octects are blocked , remaining 1 octect available to use
    ( 3 octects also refers 8*3 = 24 == in use ,   1 Octect = 8*1 = 8   == available to use )
    ( so 2**8 Ip adressess available to use  == 256 ipaddress )
  
  classless inter domain routing = reduces wastage of Ip addresses by providing exact no of ip addressess to users 
   - 172.31.1.0/28  ==  [ 32-28 (28 bits locked, 4 can be used) so  2**4  == 16 Ipaddress available ]

  Example
   - we Divide VPC Into smaller chunks called subnets called public and private
    ( These are bounded in a availability zone )
    - 10.0.0.0/16 = vpc cidr block    so 2 octects locked and 2 can be used == /24
    |  --  10.0.0.0/24   ~ subnet 1 
    |  --  10.0.1.0/24   ~ subnet 2

  Process 
    - Created Vpc in a Region with cidr block 
      > seperated 2 subnets in AZ with that cidr block 
      > created 2 route table each > added IGW > edited route of iGW to public subnet 
      > added 2 ec2 in seperate Subnets ( assign public ip to public subnet )
      > Access Public subnet EC2 using public ipv4 address

   
- Security groups -                 applied on instance levels for routing traffic
- Elastic Ips  - fixed Ip addresses applied on instances that dont vanish after restarting ec2's

- Internet Gateway IGW  attach to subnet for external trafic = called public subnet
- Route table are rules specified to allow or deny any traffic - created for the subnets 
  ( seperatly for private and public subnets and then associate with subnets )
  [ Destination will be 0.0.0.0.0 ] => means from anywhere 
  ( adding internet gateway to subnet as target )
  (  NAT gatway added to private subnet  )
   
- NAT gateway placed in public subnet, allow external traffic for instances in private subnet
  - is a one way communication , private subnet -> internet,   here internet can communicate back !
- NACL  applies on subnet level - allowing traffic or denying it 


- 5 vpc per account , 200 subnets per vpc , total 1000 subnets per account
- Vpc peering  = connection between vpc having different cidr blocks
- Vpc Endpoint = private connection btw ur vpc and other AWS services without internet

- Vpn private network = is a service to have secure connection btw ur Vpc to on-premises through IPSec protocol .
   - here internet is required
   - for small workloads
   - only components inside VPC can be used 
   - internet is  not stable 

- Direct Network ( much better than VPN ) = direct connect gateway is a network service to connect on premises to aws  without internet 
   - here internet is NOT  required
   - for large workloads
   - all aws resources can be used 
   - internet is stable 


Q  Access to Private instance in Private subnet ?
  -  logint to public ec2 > create sample.pem file and copy paste key pair inside public ec2 machine 
  > chmod 400 sample.pem  >  ssh -i sample.pem ec2-user@<private_ip_address>
  ( so private ec2 cant access google using ping , we use NAT gatways to allow private subnets to access internet )


IAM

  Identy Access management = Limiting AWS resources to specific users 
  Iam user     = Unique user based on their capability and their permission to access AWS resources will be different 
  Iam group    = combine multiple iam users inorder to attach or detach iam policies for permissions
  Iam Role     = permission initated to Aws resources so that they can directly access resources without access key or secret access ID 
                | Ex -  Your web app on ec2 needs to access s3 to store images, attach Iam role with S3FullAccess pre-defined policy
  Iam policies = Permission can be pre defined or custom made attached to Iam user, Iam groups, Iam roles 
                | in custom made > if resources defined are Allow and deny both > then Iam Identy can Access that resource

Lambda

  provide serverless compute resources basis on API Calls
  - no server required , its serverless architecture - solution to call ur functions as Lambda API 
  ( just apply your code to the lambda )
  - Automatic Scaling based on size of workload 
  - payment marked on amount of time code is runnning 
  - Example - Adding images to s3 bucket > aws lambda triggered to verify the images and convert to thumbnails > database
  - Example 2 = client hit request > lambda triggered > based on load > auto scale docker containers
  - Example 3 = copy ongoing data changes  from source S3 bucket to destination S3 bucket using aws lambda 
    |           and iam role attached on lambda where we specify source and destination bucket in IAM policy
    |           action :  s3:GetObject, s3:PutObject,  source bucket Resource: source bucket arn and destination bucket arn


Step Functions 

  - Automation for aws applications 
    - which are divided into steps to perform multiple actions on aws resources to get tasks done 
    - just like python script created to run aws resources automatically
    - can be used as alert scheduler via SNS or SQS, run parallel tasks with lambda , storage in S3
    
  | Example 1 =  Scenerio where if user purchase succedded return user with "purchase" message
    |       - 2 lambda functions for purachase confirmation and purachase refund set by user 
    |       > invoke Iam role on Step function and by default it comes with invoking lambda function
    |       > created state machine ( step function ) > initialized code to perform based on input type$.
    |       > attach iam role created to step function > create state machine > Start Execution > give inputs >>> executed Lambda

  | Example 2 - If giving SQS URL or SNS ARN in step functions json code  = it will automatically detect the IAM role for you when you create this step function
    | for SQS - we give sqs URL to attach in step function code and provide SNS ARN for   aws SNS    |

  | Example 3 - can perform Retry operations too if Tasks of function name fails to add task in Dynomodb table
    
      "Comment":{"description of the step function to Understand it "}
      "StartsAt":{" from which function name u wanna start at ! "}
      "states" :{'stores all function names', 'type inluded','Next: what to do next' }
      |           Type can be 
      |                     "Pass"  - to ignore function       
      |                     "Task"  - refers to aws function      
      |                     "Map"   - Validate multiple requests 
      |                     "Wait"  : run function after waiting or particular Timestamp
      |                       "Seconds":3 or 
      |                       "Timestamp":"2022-12-22-18t00:00:00z"

      "$.TranscationID" : { $ refers to input where we give dictionary of key TranscationID and its value }
      "End": true
    
    - json code syntax

      {
        "comment":"description of step function",
        "StartsAt":"ProcessTranscation"   # starts at   function name presented under States
        "States":{
                "ProcessTranscation":{
                  "Type": "Pass",           # type can be Pass or Task - which is referred to aws resource
                  "Next": "StoreHistory"    # Next == after completion what is next function ? 
                },
                "StoreHistory":{            # function 2  - here type is task 
                  "Type": "Task",
                  "Resource": "ARN or URK",  # refers to aws resource - could be arn or url of that resource 
                  "Parameters":{
                    "TableName":"Dynomo db table name",
                    "item":{
                      "TranscationID":{
                        "S.$":"$.TranscationID"
                      }
                    }
                  }
                },
                "Retry":[
                      {
                        "ErrorEquals":[
                          "States.ALL"
                        ],
                        "IntervalSeconds":1,
                        "MaxAttempts":3
                      }
                ],
                End : true,
                "ResultPath":"$.DynomoDB"     # we will give input dictionary with key DynomoDB, here value of key is stored
        }

      }

      ---input---
      {"TranscationID":"3849fh89"}

  | Example 4 = execute step function  from lambda to send message in SQS

      |     
      |     > create json syntax which sends queue message to sqs where inputs are -
      |     > input:{"MessageBody":{'hi'},"Type":"Message"}, then copy step function ARN
      |     ( these input keys "MessageBody" , "Type" will be sent through lambda functions )
      
      |     > create lambda function from scratch , choose Step function FullAccesss Role 
      |       (use least priviledge in production where under json code > "Action":"states:StartExecution") 
      |     
      |       > lambda code - to interact with step function 
      |         import json
                import boto3
                import uuid

                client = boto3.client('stepfunctions')  

                def lambda_handler(event, context):
    
                  transactionId = str(uuid.uuid1()) 

                  input = {'TransactionId': transactionId, 'Type': 'PURCHASE'}

                  response = client.start_execution(
                      
                      stateMachineArn = 'use the copied step function ARN here ',
                      name = transactionId,
                      input = json.dumps(input)	  
                      )

      |      > Executed  lambda  
      |      >>> which in turn executes "Step function State machine" 
      |      >>> sends message with TransactionId to SQS
      |      >   BEtter to use AWS workflow for crearting State machines



    